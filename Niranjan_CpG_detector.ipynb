{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fasta_reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8QkQAvzUuCn",
        "outputId": "57c69444-c386-4994-e804-a6362819f257"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasta_reader in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fasta_reader) (2023.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BRCiuALH7Uee"
      },
      "outputs": [],
      "source": [
        "from fasta_reader import read_fasta\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions\n",
        "\n",
        "\n",
        "def count_cg(dna_sequence: str) -> int:\n",
        "\n",
        "    \"\"\" This function iterates through a DNA sequence string to count occurrences of the \"CG\" dinucleotide.\n",
        "    It's a straightforward approach that does not require any external libraries.\"\"\"\n",
        "\n",
        "    cg_count = 0\n",
        "\n",
        "    # Iterate over the sequence to find 'CG'\n",
        "    for i in range(len(dna_sequence) - 1):  # Subtract 1 to avoid index out of range\n",
        "        if dna_sequence[i:i+2] == \"CG\":  # Check if the current and next character form a 'CG' pair\n",
        "            cg_count += 1\n",
        "\n",
        "    return cg_count\n",
        "\n",
        "\n",
        "\n",
        "def sequence_into_chunks_loop(sequence: str, chunk_size=800, sequence_size=128)-> list:\n",
        "    \"\"\"This function divides a long DNA sequence into smaller chunks of a specified size\n",
        "    (default is 128). It's useful for processing large sequences in manageable pieces.\"\"\"\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0,chunk_size*sequence_size, sequence_size):\n",
        "        chunks.append(sequence[i:i + sequence_size])\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_dataset_with_cg_counts(sequence: str, chunk_size=800, sequence_size=128) -> list:\n",
        "\n",
        "    \"\"\"Utilizes sequence_into_chunks_loop to first slice the sequence into chunks and then applies count_cg to each chunk to\n",
        "  create a dataset where each item is a tuple containing a chunk and its corresponding \"CG\" count.\"\"\"\n",
        "    # Use the provided function to slice the sequence into chunks\n",
        "    chunks = sequence_into_chunks_loop(sequence, chunk_size,sequence_size)\n",
        "\n",
        "    # Create a new dataset by counting 'CG' in each chunk\n",
        "    # Each element in the new dataset is a tuple (chunk, cg_count)\n",
        "    dataset = [(chunk, count_cg(chunk)) for chunk in chunks]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "vocab = ['N', 'A', 'C', 'G', 'T']  # Include 'N' for unknown nucleotides\n",
        "vocab_size = len(vocab)\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def sequence_to_indices(sequence:str, char_to_index):\n",
        "\n",
        "    \"\"\"Converts a DNA sequence into a list of numerical indices based on a predefined mapping (char_to_index).\n",
        "    This step is crucial for transforming categorical nucleotide data into a numerical format that machine learning models\n",
        "    can process.\"\"\"\n",
        "\n",
        "    return [char_to_index[char] for char in sequence]\n",
        "\n",
        "\n",
        "def sequence_to_indices_chunk(sequence:str, chunk_size:int,sequence_size:int,char_to_index)->list :\n",
        "\n",
        "    \"\"\" convert sequence to mapped indices\"\"\"\n",
        "    chunks = sequence_into_chunks_loop(sequence, chunk_size,sequence_size)\n",
        "\n",
        "    chunk_indices=[]\n",
        "    for i in range(len(chunks)):\n",
        "      row=sequence_to_indices(chunks[i],char_to_index)\n",
        "      chunk_indices.append(list(row))\n",
        "    return chunk_indices\n",
        "\n",
        "def sequence_to_indeces_dataset(sequence: str, chunk_size=800, sequence_size=128)-> list:\n",
        "    \"\"\"convert a sequence into a dataset that has label appended in the end \"\"\"\n",
        "    chunks = sequence_into_chunks_loop(sequence, chunk_size,sequence_size)\n",
        "    chunks_ind= sequence_to_indices_chunk (cleaned_sequence,chunk_size,sequence_size,char_to_index)\n",
        "    dataset=[]\n",
        "    for i in range(chunk_size):\n",
        "      dataset+=[(chunks_ind[i],count_cg(chunks[i]))]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#pad the sequence and convert to tensor\n",
        "\n",
        "def pad_and_retrieve_counts(dataset):\n",
        "    \"\"\"\n",
        "    Pad the sequences represented as lists of indices to uniform length and retrieve CG counts.\n",
        "\n",
        "    Args:\n",
        "    - dataset (list of tuples): Each tuple contains a sequence as a list of indices and its \"CG\" count.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Padded sequences.\n",
        "    - Tensor: Corresponding \"CG\" counts.\n",
        "    \"\"\"\n",
        "    # Unpack sequences and counts from the dataset\n",
        "    sequence_tensors = [torch.tensor(seq) for seq, _ in dataset]\n",
        "    cg_counts = torch.tensor([count for _, count in dataset])\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_sequences = pad_sequence(sequence_tensors, batch_first=True)\n",
        "\n",
        "    return padded_sequences, cg_counts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PJMT804IEuUK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE =\"/content/drive/MyDrive/sequence.fasta\"\n",
        "for item in read_fasta(FILE):\n",
        "  continue\n",
        "sequence=item.sequence"
      ],
      "metadata": {
        "id": "LEvof9aKUtaC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sequence=''.join(filter(lambda char: char in ('A', 'C', 'G', 'T'), sequence))\n",
        "dataset_i=sequence_to_indeces_dataset(cleaned_sequence)\n",
        "padded_sequences,cg_counts=pad_and_retrieve_counts(dataset_i)"
      ],
      "metadata": {
        "id": "yVGw-xxOVaFd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Data for Neural Networks\n"
      ],
      "metadata": {
        "id": "-zgJSBOulsWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING SET , VALIDATION SET , TEST SET\n",
        "total_samples = 800 # Should be 500 based on your description\n",
        "train_size = 400\n",
        "val_size = 200\n",
        "test_size = 200  # Total = 500\n",
        "\n",
        "dataset = TensorDataset(padded_sequences, cg_counts.float())\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size_train = 1  # Example batch size for training; adjust as needed\n",
        "batch_size_val_test = 1  # Batch size for validation and testing\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size_val_test, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_val_test, shuffle=False)\n"
      ],
      "metadata": {
        "id": "zgmSwNQgFZIo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Prep"
      ],
      "metadata": {
        "id": "OxTcg-FAyZqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNALSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1):\n",
        "        super(DNALSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        # You might want to take the output from the last timestep or aggregate outputs\n",
        "        final_feature_map = lstm_out[:, -1, :]\n",
        "        output = self.fc(final_feature_map)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "UrGvLV37t8G2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model__(model, data_loader, loss_function, optimizer, device, num_epochs=100):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences).squeeze()  # Adjust based on your model's output\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()  # Accumulate the loss\n",
        "            num_batches += 1\n",
        "\n",
        "        # Calculate the average loss over all batches\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "xfNOG9JJDM_q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # No need to track gradients during evaluation\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            predictions = model(sequences)\n",
        "            loss = loss_function(predictions, labels.float())  # Assuming regression; use .float() for labels if needed\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)  # Calculate average loss\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "8XKPec-qFIyQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 248\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Assuming a regression task; adjust as needed for classification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DNALSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model = model.to(device)\n",
        "loss_function = nn.MSELoss()  # For regression\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "OOt4PhyIwXrc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "train_model__(model, train_loader, loss_function, optimizer,\"cuda\")\n"
      ],
      "metadata": {
        "id": "ZvItHUC67lyP",
        "outputId": "aa53e5ef-95ae-4d52-9bcf-b51550e2c3a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Average Loss: 6.0266\n",
            "Epoch 2/100, Average Loss: 5.3361\n",
            "Epoch 3/100, Average Loss: 4.8609\n",
            "Epoch 4/100, Average Loss: 4.3229\n",
            "Epoch 5/100, Average Loss: 3.9718\n",
            "Epoch 6/100, Average Loss: 3.7731\n",
            "Epoch 7/100, Average Loss: 3.7577\n",
            "Epoch 8/100, Average Loss: 3.6823\n",
            "Epoch 9/100, Average Loss: 3.7130\n",
            "Epoch 10/100, Average Loss: 3.6933\n",
            "Epoch 11/100, Average Loss: 2.9640\n",
            "Epoch 12/100, Average Loss: 2.4298\n",
            "Epoch 13/100, Average Loss: 2.2809\n",
            "Epoch 14/100, Average Loss: 2.0299\n",
            "Epoch 15/100, Average Loss: 1.9565\n",
            "Epoch 16/100, Average Loss: 1.5955\n",
            "Epoch 17/100, Average Loss: 1.4113\n",
            "Epoch 18/100, Average Loss: 1.4297\n",
            "Epoch 19/100, Average Loss: 1.6051\n",
            "Epoch 20/100, Average Loss: 1.8878\n",
            "Epoch 21/100, Average Loss: 1.5462\n",
            "Epoch 22/100, Average Loss: 1.4591\n",
            "Epoch 23/100, Average Loss: 1.3893\n",
            "Epoch 24/100, Average Loss: 1.1796\n",
            "Epoch 25/100, Average Loss: 1.1997\n",
            "Epoch 26/100, Average Loss: 0.9650\n",
            "Epoch 27/100, Average Loss: 1.1245\n",
            "Epoch 28/100, Average Loss: 0.7869\n",
            "Epoch 29/100, Average Loss: 0.8003\n",
            "Epoch 30/100, Average Loss: 0.7950\n",
            "Epoch 31/100, Average Loss: 0.5878\n",
            "Epoch 32/100, Average Loss: 0.8168\n",
            "Epoch 33/100, Average Loss: 0.5650\n",
            "Epoch 34/100, Average Loss: 0.5414\n",
            "Epoch 35/100, Average Loss: 0.4600\n",
            "Epoch 36/100, Average Loss: 0.7729\n",
            "Epoch 37/100, Average Loss: 0.9751\n",
            "Epoch 38/100, Average Loss: 1.0345\n",
            "Epoch 39/100, Average Loss: 0.6575\n",
            "Epoch 40/100, Average Loss: 0.5093\n",
            "Epoch 41/100, Average Loss: 0.7369\n",
            "Epoch 42/100, Average Loss: 0.6774\n",
            "Epoch 43/100, Average Loss: 0.4563\n",
            "Epoch 44/100, Average Loss: 0.4001\n",
            "Epoch 45/100, Average Loss: 0.4688\n",
            "Epoch 46/100, Average Loss: 0.5088\n",
            "Epoch 47/100, Average Loss: 0.4069\n",
            "Epoch 48/100, Average Loss: 0.3349\n",
            "Epoch 49/100, Average Loss: 0.2500\n",
            "Epoch 50/100, Average Loss: 0.2219\n",
            "Epoch 51/100, Average Loss: 0.2270\n",
            "Epoch 52/100, Average Loss: 0.2005\n",
            "Epoch 53/100, Average Loss: 0.2442\n",
            "Epoch 54/100, Average Loss: 0.2269\n",
            "Epoch 55/100, Average Loss: 0.2568\n",
            "Epoch 56/100, Average Loss: 0.1606\n",
            "Epoch 57/100, Average Loss: 0.1473\n",
            "Epoch 58/100, Average Loss: 0.1777\n",
            "Epoch 59/100, Average Loss: 0.1884\n",
            "Epoch 60/100, Average Loss: 0.1940\n",
            "Epoch 61/100, Average Loss: 0.1850\n",
            "Epoch 62/100, Average Loss: 0.4035\n",
            "Epoch 63/100, Average Loss: 0.2475\n",
            "Epoch 64/100, Average Loss: 0.3821\n",
            "Epoch 65/100, Average Loss: 0.2610\n",
            "Epoch 66/100, Average Loss: 0.2400\n",
            "Epoch 67/100, Average Loss: 0.1487\n",
            "Epoch 68/100, Average Loss: 0.1149\n",
            "Epoch 69/100, Average Loss: 0.2098\n",
            "Epoch 70/100, Average Loss: 0.1178\n",
            "Epoch 71/100, Average Loss: 0.1061\n",
            "Epoch 72/100, Average Loss: 0.1012\n",
            "Epoch 73/100, Average Loss: 0.0852\n",
            "Epoch 74/100, Average Loss: 0.0962\n",
            "Epoch 75/100, Average Loss: 0.1316\n",
            "Epoch 76/100, Average Loss: 0.2856\n",
            "Epoch 77/100, Average Loss: 0.2821\n",
            "Epoch 78/100, Average Loss: 0.1575\n",
            "Epoch 79/100, Average Loss: 0.1303\n",
            "Epoch 80/100, Average Loss: 0.1149\n",
            "Epoch 81/100, Average Loss: 0.1574\n",
            "Epoch 82/100, Average Loss: 0.1336\n",
            "Epoch 83/100, Average Loss: 0.1210\n",
            "Epoch 84/100, Average Loss: 0.1380\n",
            "Epoch 85/100, Average Loss: 0.1181\n",
            "Epoch 86/100, Average Loss: 0.0924\n",
            "Epoch 87/100, Average Loss: 0.0944\n",
            "Epoch 88/100, Average Loss: 0.0853\n",
            "Epoch 89/100, Average Loss: 0.4240\n",
            "Epoch 90/100, Average Loss: 0.1369\n",
            "Epoch 91/100, Average Loss: 0.2219\n",
            "Epoch 92/100, Average Loss: 0.0934\n",
            "Epoch 93/100, Average Loss: 0.0931\n",
            "Epoch 94/100, Average Loss: 0.0969\n",
            "Epoch 95/100, Average Loss: 0.0978\n",
            "Epoch 96/100, Average Loss: 0.0758\n",
            "Epoch 97/100, Average Loss: 0.0719\n",
            "Epoch 98/100, Average Loss: 0.0929\n",
            "Epoch 99/100, Average Loss: 0.0696\n",
            "Epoch 100/100, Average Loss: 0.0507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/cpg_detector_lstm.pth\")\n"
      ],
      "metadata": {
        "id": "nWp0bmgAOgsk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model\n"
      ],
      "metadata": {
        "id": "rSKuc5LRFlN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "TuxyBglNEevC",
        "outputId": "bae3089d-3eab-49b2-ed40-6f9311a7d441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.1885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, val_loader, device)"
      ],
      "metadata": {
        "id": "VrJI6Xc2FC7b",
        "outputId": "42e37d07-688a-4c27-fe09-673d9fa3eb6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.6296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD MODEL HERE"
      ],
      "metadata": {
        "id": "RN4NPYbFOGOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_new = DNALSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model_new.load_state_dict(torch.load(\"/content/drive/MyDrive/cpg_detector_lstm.pth\"))\n",
        "model_new.to(device)\n"
      ],
      "metadata": {
        "id": "Vi5w-lUIIl_y",
        "outputId": "2e1fdf30-1160-4d49-a914-caa2c5a80317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNALSTM(\n",
              "  (embedding): Embedding(5, 248)\n",
              "  (lstm): LSTM(248, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model_new, test_loader, device)"
      ],
      "metadata": {
        "id": "an9Bg1tTMU9F",
        "outputId": "c7f9eefe-7371-4278-da33-644908fb55bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.1885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA4VbQsmRXlw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}