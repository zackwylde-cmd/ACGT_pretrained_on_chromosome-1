{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: Data Processing\n",
        "\n",
        "1. we are using the DNA sequence of chromosome 1 for this project\n",
        "\n",
        "2. find the dataset here : https://www.ncbi.nlm.nih.gov/nuccore/NC_000001.11?report=fasta\n",
        "\n",
        "\n",
        "3. The dataset contains characters ('N',\n",
        "'A', 'C', 'G', 'T') :- hence we require data preprocessing\n",
        "\n",
        "4. the preprocessing code contains char to index mapping to enable character embedding\n",
        "\n",
        "5. we are creating a dataset from a fasta document\n",
        "\n"
      ],
      "metadata": {
        "id": "qODT3sPRbLMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-689yXEaEbW"
      },
      "outputs": [],
      "source": [
        "!pip install fasta_reader\n",
        "!pip install gradio==3.50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fasta_reader import read_fasta\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "6dTi8P-DaiAR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper Functions\n",
        "\n",
        "\n",
        "def count_cg_pairs(dna_sequence: str) -> int:\n",
        "\n",
        "  \"\"\"\n",
        "  Count the occurrences of the \"CG\" dinucleotide within a given DNA sequence.\n",
        "\n",
        "  Parameters:\n",
        "  - dna_sequence (str): A string representing the DNA sequence to be analyzed.\n",
        "\n",
        "  Returns:\n",
        "  - int: The total count of \"CG\" dinucleotides found in the DNA sequence.\n",
        "  \"\"\"\n",
        "  cg_count = 0\n",
        "\n",
        "  # Iterate over the sequence to find 'CG'\n",
        "  for i in range(len(dna_sequence) - 1):  # Subtract 1 to avoid index out of range\n",
        "      if dna_sequence[i:i+2] == \"CG\":  # Check if the current and next character form a 'CG' pair\n",
        "          cg_count += 1\n",
        "\n",
        "  return cg_count\n",
        "\n",
        "\n",
        "#-\n",
        "def dna_sequence_into_chunks(dna_sequence: str, chunk_size=800, sequence_size=128)-> list:\n",
        "  \"\"\"\n",
        "  Divide a DNA sequence into smaller chunks of a specified size.\n",
        "\n",
        "  Parameters:\n",
        "  - dna_sequence (str): The DNA sequence to be divided into chunks.\n",
        "  - chunk_size (int): The total number of chunks to produce. This parameter primarily serves to control the loop and is not directly related to the final number of chunks if the sequence length does not perfectly divide by 'sequence_size'.\n",
        "  - sequence_size (int): The desired size of each chunk. Default is 128 characters.\n",
        "\n",
        "  Returns:\n",
        "  - list: A list of string chunks, each of the specified 'sequence_size'.\n",
        "  \"\"\"\n",
        "\n",
        "  chunks = []\n",
        "  for i in range(0,chunk_size*sequence_size, sequence_size):\n",
        "      chunks.append(dna_sequence[i:i + sequence_size])\n",
        "  return chunks\n",
        "\n",
        "\n",
        "def dna_sequence_to_indices(sequence:str, char_to_index):\n",
        "\n",
        "  \"\"\"\n",
        "  Converts a DNA sequence into a list of numerical indices based on a predefined mapping (char_to_index).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  return [char_to_index[char] for char in sequence]\n",
        "\n",
        "def dna_sequence_to_indices_chunk(dna_sequence:str, chunk_size:int,sequence_size:int,char_to_index)->list :\n",
        "\n",
        "    \"\"\" convert sequence to mapped indices\"\"\"\n",
        "    chunks = dna_sequence_into_chunks(dna_sequence, chunk_size,sequence_size)\n",
        "\n",
        "    chunk_indices=[]\n",
        "    for i in range(len(chunks)):\n",
        "      row=dna_sequence_to_indices(chunks[i],char_to_index)\n",
        "      chunk_indices.append(list(row))\n",
        "    return chunk_indices\n",
        "\n",
        "def dna_sequence_to_indeces_dataset(sequence: str, chunk_size=800, sequence_size=128)-> list:\n",
        "    \"\"\"\n",
        "    Convert a DNA sequence into a dataset with each chunk labeled by its 'CG' count.\n",
        "\n",
        "    Parameters:\n",
        "    - sequence (str): The full DNA sequence to be processed.\n",
        "    - chunk_size (int): Controls the number of chunks and data points generated.\n",
        "    - sequence_size (int): Determines the uniform size of each chunk, in characters.\n",
        "\n",
        "    Returns:\n",
        "    - list: Dataset of tuples (chunk as indices, 'CG' count), ready for computational analysis and modeling.\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "    chunks = dna_sequence_into_chunks(sequence, chunk_size,sequence_size)\n",
        "    chunks_ind= dna_sequence_to_indices_chunk (cleaned_sequence,chunk_size,sequence_size,char_to_index)\n",
        "    dataset=[]\n",
        "    for i in range(chunk_size):\n",
        "      dataset+=[(chunks_ind[i],count_cg_pairs(chunks[i]))]\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "xTgG1VyXdiuT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the file and cleaning the DNA sequence\n",
        "\n",
        "FILE =\"/content/drive/MyDrive/sequence.fasta\"\n",
        "for item in read_fasta(FILE):\n",
        "  continue\n",
        "sequence=item.sequence\n",
        "\n",
        "\"\"\"\n",
        "The dataset should only contain characters ('N',\n",
        "'A', 'C', 'G', 'T','R') :- hence we require data preprocessing\n",
        "\"\"\"\n",
        "\n",
        "cleaned_sequence=''.join(filter(lambda char: char in ('A', 'C', 'G', 'T'), sequence))\n",
        "\n"
      ],
      "metadata": {
        "id": "7tH29XjnajVS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we use character indexing so that we can use character embedding\n",
        "vocab = ['N', 'A', 'C', 'G', 'T']  # Include 'N' for unknown nucleotides\n",
        "vocab_size = len(vocab)\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NO2boOj-dWzY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dataset\n",
        "dataset_i=dna_sequence_to_indeces_dataset(cleaned_sequence)\n"
      ],
      "metadata": {
        "id": "rp_kZQYHsC_Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Padding the dna sequence and converting to tensor\n"
      ],
      "metadata": {
        "id": "hkde0KftpzbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dna_sequence_to_tensor(sequence, char_to_index=char_to_index, device='cuda'):\n",
        "  \"\"\"\n",
        "  Convert a DNA sequence to a PyTorch tensor based on a character-to-index mapping.\n",
        "\n",
        "  \"\"\"\n",
        "  indices = [char_to_index[char] for char in sequence]\n",
        "\n",
        "  sequence_tensor = torch.tensor(indices, dtype=torch.long, device=device)\n",
        "\n",
        "  sequence_tensor = sequence_tensor.unsqueeze(0)\n",
        "\n",
        "  return sequence_tensor\n",
        "\n",
        "\n",
        "def pad_and_retrieve_counts(dataset):\n",
        "  \"\"\"\n",
        "  Pad the sequences represented as lists of indices to uniform length and retrieve CG counts.\n",
        "\n",
        "  Args:\n",
        "  - dataset (list of tuples): Each tuple contains a sequence as a list of indices and its \"CG\" count.\n",
        "\n",
        "  Returns:\n",
        "  - Tensor: Padded sequences.\n",
        "  - Tensor: Corresponding \"CG\" counts.\n",
        "  \"\"\"\n",
        "  # Unpack sequences and counts from the dataset\n",
        "  dna_sequence_to_tensor = [torch.tensor(seq) for seq, _ in dataset]\n",
        "  cg_counts = torch.tensor([count for _, count in dataset])\n",
        "\n",
        "  # Pad the sequences\n",
        "  padded_sequences = pad_sequence(dna_sequence_to_tensor, batch_first=True)\n",
        "\n",
        "  return padded_sequences, cg_counts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vxqprQqTdWna"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding the sequence\n",
        "\n",
        "padded_sequences,cg_counts=pad_and_retrieve_counts(dataset_i)"
      ],
      "metadata": {
        "id": "dbODBs9dsx-A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data loader\n",
        "  - set train size, validation size, test size\n",
        "  - set batch size"
      ],
      "metadata": {
        "id": "j4je7OM0uVdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING SET , VALIDATION SET , TEST SET\n",
        "total_samples = 800 # Should be 500 based on your description\n",
        "train_size = 400\n",
        "val_size = 200\n",
        "test_size = 200  # Total = 500\n",
        "\n",
        "dataset = TensorDataset(padded_sequences, cg_counts.float())\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size_train = 1  # Example batch size for training; adjust as needed\n",
        "batch_size_val_test = 1  # Batch size for validation and testing\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size_val_test, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_val_test, shuffle=False)\n"
      ],
      "metadata": {
        "id": "lfD3f_thucOQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Config\n",
        "  - set model config (contains embedding layer)\n",
        "  - set loss Function\n",
        "  - set optimizer\n",
        "  - set embedding dimension\n",
        "  - set hidden output"
      ],
      "metadata": {
        "id": "colWNbnpugZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNALSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1):\n",
        "        super(DNALSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        # You might want to take the output from the last timestep or aggregate outputs\n",
        "\n",
        "        final_feature_map = lstm_out[:, -1, :]\n",
        "        output = self.fc(final_feature_map)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "9KOHqEMLvSrI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model Config\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 248\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Assuming a regression task; adjust as needed for classification\n"
      ],
      "metadata": {
        "id": "wUXZOp0eufDl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the LSTM model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DNALSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model = model.to(device)\n",
        "loss_function = nn.MSELoss()  # For regression\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "14m5ykvGvV4r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "  - set number of epoch here"
      ],
      "metadata": {
        "id": "WMt1RU43vvXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model__(model, data_loader, loss_function, optimizer, device, num_epochs=100):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences).squeeze()  # Adjust based on your model's output\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()  # Accumulate the loss\n",
        "            num_batches += 1\n",
        "\n",
        "        # Calculate the average loss over all batches\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "fXB2WCT_v0k0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "train_model__(model, train_loader, loss_function, optimizer,\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BeyKbUJv-q7",
        "outputId": "1cae2db3-e1c3-44fa-cdfa-a102aaa1952e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Average Loss: 7.3681\n",
            "Epoch 2/100, Average Loss: 5.9913\n",
            "Epoch 3/100, Average Loss: 5.6659\n",
            "Epoch 4/100, Average Loss: 5.2249\n",
            "Epoch 5/100, Average Loss: 4.8205\n",
            "Epoch 6/100, Average Loss: 4.1057\n",
            "Epoch 7/100, Average Loss: 3.7829\n",
            "Epoch 8/100, Average Loss: 3.5796\n",
            "Epoch 9/100, Average Loss: 3.3339\n",
            "Epoch 10/100, Average Loss: 3.5374\n",
            "Epoch 11/100, Average Loss: 3.0597\n",
            "Epoch 12/100, Average Loss: 3.1327\n",
            "Epoch 13/100, Average Loss: 3.7372\n",
            "Epoch 14/100, Average Loss: 3.3960\n",
            "Epoch 15/100, Average Loss: 2.6874\n",
            "Epoch 16/100, Average Loss: 2.5967\n",
            "Epoch 17/100, Average Loss: 2.3733\n",
            "Epoch 18/100, Average Loss: 1.9852\n",
            "Epoch 19/100, Average Loss: 1.6415\n",
            "Epoch 20/100, Average Loss: 1.7117\n",
            "Epoch 21/100, Average Loss: 1.4361\n",
            "Epoch 22/100, Average Loss: 1.0767\n",
            "Epoch 23/100, Average Loss: 1.3426\n",
            "Epoch 24/100, Average Loss: 1.0264\n",
            "Epoch 25/100, Average Loss: 0.8859\n",
            "Epoch 26/100, Average Loss: 0.9148\n",
            "Epoch 27/100, Average Loss: 0.8143\n",
            "Epoch 28/100, Average Loss: 0.7718\n",
            "Epoch 29/100, Average Loss: 1.2540\n",
            "Epoch 30/100, Average Loss: 0.7138\n",
            "Epoch 31/100, Average Loss: 0.6022\n",
            "Epoch 32/100, Average Loss: 0.8072\n",
            "Epoch 33/100, Average Loss: 0.6959\n",
            "Epoch 34/100, Average Loss: 0.6362\n",
            "Epoch 35/100, Average Loss: 0.6358\n",
            "Epoch 36/100, Average Loss: 0.8441\n",
            "Epoch 37/100, Average Loss: 0.6157\n",
            "Epoch 38/100, Average Loss: 0.4863\n",
            "Epoch 39/100, Average Loss: 0.5548\n",
            "Epoch 40/100, Average Loss: 0.6613\n",
            "Epoch 41/100, Average Loss: 0.4532\n",
            "Epoch 42/100, Average Loss: 0.7446\n",
            "Epoch 43/100, Average Loss: 0.5283\n",
            "Epoch 44/100, Average Loss: 0.3657\n",
            "Epoch 45/100, Average Loss: 0.2429\n",
            "Epoch 46/100, Average Loss: 0.1897\n",
            "Epoch 47/100, Average Loss: 0.1714\n",
            "Epoch 48/100, Average Loss: 0.4943\n",
            "Epoch 49/100, Average Loss: 0.5705\n",
            "Epoch 50/100, Average Loss: 0.3543\n",
            "Epoch 51/100, Average Loss: 0.2824\n",
            "Epoch 52/100, Average Loss: 0.2635\n",
            "Epoch 53/100, Average Loss: 0.1863\n",
            "Epoch 54/100, Average Loss: 0.1861\n",
            "Epoch 55/100, Average Loss: 0.2364\n",
            "Epoch 56/100, Average Loss: 0.1822\n",
            "Epoch 57/100, Average Loss: 0.2455\n",
            "Epoch 58/100, Average Loss: 0.1882\n",
            "Epoch 59/100, Average Loss: 0.1239\n",
            "Epoch 60/100, Average Loss: 0.1773\n",
            "Epoch 61/100, Average Loss: 0.1599\n",
            "Epoch 62/100, Average Loss: 0.1227\n",
            "Epoch 63/100, Average Loss: 0.1095\n",
            "Epoch 64/100, Average Loss: 0.1353\n",
            "Epoch 65/100, Average Loss: 0.1255\n",
            "Epoch 66/100, Average Loss: 0.1025\n",
            "Epoch 67/100, Average Loss: 0.1304\n",
            "Epoch 68/100, Average Loss: 0.1653\n",
            "Epoch 69/100, Average Loss: 0.2542\n",
            "Epoch 70/100, Average Loss: 0.3940\n",
            "Epoch 71/100, Average Loss: 0.3485\n",
            "Epoch 72/100, Average Loss: 0.1098\n",
            "Epoch 73/100, Average Loss: 0.1645\n",
            "Epoch 74/100, Average Loss: 0.1732\n",
            "Epoch 75/100, Average Loss: 0.1007\n",
            "Epoch 76/100, Average Loss: 0.0697\n",
            "Epoch 77/100, Average Loss: 0.0696\n",
            "Epoch 78/100, Average Loss: 0.0654\n",
            "Epoch 79/100, Average Loss: 0.0743\n",
            "Epoch 80/100, Average Loss: 0.1270\n",
            "Epoch 81/100, Average Loss: 0.1024\n",
            "Epoch 82/100, Average Loss: 0.1033\n",
            "Epoch 83/100, Average Loss: 0.1213\n",
            "Epoch 84/100, Average Loss: 0.1058\n",
            "Epoch 85/100, Average Loss: 0.0697\n",
            "Epoch 86/100, Average Loss: 0.0708\n",
            "Epoch 87/100, Average Loss: 0.1661\n",
            "Epoch 88/100, Average Loss: 0.1582\n",
            "Epoch 89/100, Average Loss: 0.1233\n",
            "Epoch 90/100, Average Loss: 0.3744\n",
            "Epoch 91/100, Average Loss: 0.1634\n",
            "Epoch 92/100, Average Loss: 0.1583\n",
            "Epoch 93/100, Average Loss: 0.2144\n",
            "Epoch 94/100, Average Loss: 0.0609\n",
            "Epoch 95/100, Average Loss: 0.0458\n",
            "Epoch 96/100, Average Loss: 0.0431\n",
            "Epoch 97/100, Average Loss: 0.0472\n",
            "Epoch 98/100, Average Loss: 0.0493\n",
            "Epoch 99/100, Average Loss: 0.0804\n",
            "Epoch 100/100, Average Loss: 0.0613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save or Load model\n",
        "  - First snippet contains code to save model\n",
        "  - Second snippet contains code to load model"
      ],
      "metadata": {
        "id": "QdT1GQ_wwJ10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save model here"
      ],
      "metadata": {
        "id": "7qK-5D57waH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/cpg_detector_lstm.pth\")\n"
      ],
      "metadata": {
        "id": "R0bN58zZwNFr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model here"
      ],
      "metadata": {
        "id": "3240Gj3lwSfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "embedding_dim = 248\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Assuming a regression task; adjust as needed for classification\n",
        "\n",
        "model_new = DNALSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model_new.load_state_dict(torch.load(\"/content/drive/MyDrive/cpg_detector_lstm.pth\"))\n",
        "model_new.to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqV_wU6lwUd2",
        "outputId": "ba5ab0c0-9110-41b9-dc3b-fc46baae5a1c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNALSTM(\n",
              "  (embedding): Embedding(5, 248)\n",
              "  (lstm): LSTM(248, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "  - choose a model and data loader for evaluation\n",
        "  - we use average loss here"
      ],
      "metadata": {
        "id": "rSzMVKq9wgSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # No need to track gradients during evaluation\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            predictions = model(sequences)\n",
        "            loss = loss_function(predictions, labels.float())  # Assuming regression; use .float() for labels if needed\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)  # Calculate average loss\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "PzfqOEYbwjIl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he0IzUJLwxN0",
        "outputId": "87edaba7-72b6-421a-a435-2341d45d4e68"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.9743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2: Loading trained model and using Gradio app\n",
        "  - Contains helper function for Gradio"
      ],
      "metadata": {
        "id": "ltSlDcZ0w5Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing a trained model\n",
        "\n",
        "vocab_size = 5\n",
        "embedding_dim = 248\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Assuming a regression task; adjust as needed for classification\n",
        "\n",
        "model_new = DNALSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model_new.load_state_dict(torch.load(\"/content/drive/MyDrive/cpg_detector_lstm.pth\"))\n",
        "model_new.to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx7rlLPX7jyP",
        "outputId": "4ceed35a-bb93-4c7d-ea01-491869c882d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNALSTM(\n",
              "  (embedding): Embedding(5, 248)\n",
              "  (lstm): LSTM(248, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "#Helper fucntion for gradio interface\n",
        "def predict_cg_count(sequence) -> int :\n",
        "    \"\"\"\n",
        "    Predicts 'CG' count in a DNA sequence using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "    - sequence (str): Input DNA sequence.\n",
        "\n",
        "    Returns:\n",
        "    - int: Predicted 'CG' count.\n",
        "    \"\"\"\n",
        "    sequence_tensor = dna_sequence_to_tensor(sequence.upper(), char_to_index, )\n",
        "    with torch.no_grad():\n",
        "        prediction = model_new(sequence_tensor)\n",
        "        # Assuming your model outputs a raw count, adjust based on your model's output\n",
        "        cg_count = prediction.item()\n",
        "    return int(cg_count)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "grdEqZWKw_Qz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio interface\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_cg_count,\n",
        "    inputs=gr.Textbox(label=\"DNA Sequence Here\", placeholder=\"Enter DNA Sequence...\", lines=2),\n",
        "    outputs=gr.Number(label=\"Predicted CG Count\"),\n",
        "    title=\"DNA CG Counter\",\n",
        "    description=\"This app predicts the count of 'CG' pairs in a DNA sequence. Enter a DNA sequence to get started.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "JYPWhZMpxyvG",
        "outputId": "d698b4a4-9d70-4b32-8c42-0a0bcfad4c9d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://8f2c3f34a0d718f90c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8f2c3f34a0d718f90c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8f2c3f34a0d718f90c.gradio.live\n"
          ]
        }
      ]
    }
  ]
}